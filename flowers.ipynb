{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 16 ms (started: 2023-08-30 12:04:46 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# !pip install -q ipython-autotime\n",
    "%load_ext autoreload\n",
    "%load_ext autotime\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "File already exists\n",
      "File already exists\n",
      "File already extracted\n",
      "time: 110 ms (started: 2023-08-30 12:04:46 +03:00)\n"
     ]
    }
   ],
   "source": [
    "import requests, os, tarfile\n",
    "from tqdm import tqdm\n",
    "from data_download import download, extract_tgz\n",
    "\n",
    "dataset_url = r'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz'\n",
    "segmentations = r'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102segmentations.tgz'\n",
    "chi_distance = r'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/distancematrices102.mat'\n",
    "labels = r'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat'\n",
    "splits = r'https://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat'\n",
    "\n",
    "OVERWRITE = False\n",
    "\n",
    "download(dataset_url, dataset_url.split('/')[-1], force_redownload=OVERWRITE)\n",
    "# download(segmentations, segmentations.split('/')[-1], force_redownload=OVERWRITE)\n",
    "# download(chi_distance, chi_distance.split('/')[-1], force_redownload=OVERWRITE)\n",
    "download(labels, labels.split('/')[-1], force_redownload=OVERWRITE)\n",
    "download(splits, splits.split('/')[-1], force_redownload=OVERWRITE)\n",
    "extract_tgz(r'102flowers.tgz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.27 s (started: 2023-08-30 12:04:46 +03:00)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_processing import prepare_df, FlowerDataset\n",
    "\n",
    "\n",
    "split_path = r\"data/setid.mat\"\n",
    "labels_Path = r\"data/imagelabels.mat\"\n",
    "data_root = r\"data/102flowers/jpg\"\n",
    "train_split, test_split, val_split = prepare_df(split_path, labels_Path, data_root)\n",
    "\n",
    "transformsations = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# train_dataset = FlowerDataset(train_split, transform=transformsations)\n",
    "# test_dataset = FlowerDataset(test_split, transform=transformsations)\n",
    "# val_dataset = FlowerDataset(val_split, transform=transformsations)\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.12 s (started: 2023-08-30 12:04:47 +03:00)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchsummary\n",
    "import torch.nn as nn\n",
    "import segmentation_models_pytorch as smp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-30 12:04:49 +03:00)\n"
     ]
    }
   ],
   "source": [
    "def accuray_fn(ypred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the prediction\n",
    "    \"\"\"\n",
    "    return (ypred == y_true).sum() / len(ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns (started: 2023-08-30 12:06:20 +03:00)\n"
     ]
    }
   ],
   "source": [
    "class Resnet50Flower102(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.resnet50(pretrained=pretrained)\n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 102),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 407 ms (started: 2023-08-30 12:07:21 +03:00)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# from models  import Resnet50Flower102\n",
    "from tqdm import tqdm\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = 'cpu'\n",
    "model = Resnet50Flower102(pretrained=True).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "# optimizer = torch.optim.Adam(model.model.fc.parameters(), lr=3e-4)\n",
    "train_dataset = FlowerDataset(train_split, transform=transformsations)\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\nYou can try to repro this exception using the following code snippet. If that doesn't trigger the error, please include your original repro script when reporting this issue.\n\nimport torch\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.allow_tf32 = True\ndata = torch.randn([4, 1024, 14, 14], dtype=torch.float, device='cuda', requires_grad=True)\nnet = torch.nn.Conv2d(1024, 256, kernel_size=[1, 1], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)\nnet = net.cuda().float()\nout = net(data)\nout.backward(torch.randn_like(out))\ntorch.cuda.synchronize()\n\nConvolutionParams \n    memory_format = Contiguous\n    data_type = CUDNN_DATA_FLOAT\n    padding = [0, 0, 0]\n    stride = [1, 1, 0]\n    dilation = [1, 1, 0]\n    groups = 1\n    deterministic = false\n    allow_tf32 = true\ninput: TensorDescriptor 00000179106BD3D0\n    type = CUDNN_DATA_FLOAT\n    nbDims = 4\n    dimA = 4, 1024, 14, 14, \n    strideA = 200704, 196, 14, 1, \noutput: TensorDescriptor 00000179106BD1A0\n    type = CUDNN_DATA_FLOAT\n    nbDims = 4\n    dimA = 4, 256, 14, 14, \n    strideA = 50176, 196, 14, 1, \nweight: FilterDescriptor 00000178CAEA70F0\n    type = CUDNN_DATA_FLOAT\n    tensor_format = CUDNN_TENSOR_NCHW\n    nbDims = 4\n    dimA = 256, 1024, 1, 1, \nPointer addresses: \n    input: 0000000968E00000\n    output: 0000000968AC4000\n    weight: 0000000904A00000\nAdditional pointer addresses: \n    grad_output: 0000000968AC4000\n    grad_input: 0000000968E00000\nBackward data algorithm: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m y_hat \u001b[39m=\u001b[39m model(x)\n\u001b[0;32m      7\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(y_hat, y)\n\u001b[1;32m----> 8\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\nYou can try to repro this exception using the following code snippet. If that doesn't trigger the error, please include your original repro script when reporting this issue.\n\nimport torch\ntorch.backends.cuda.matmul.allow_tf32 = False\ntorch.backends.cudnn.benchmark = False\ntorch.backends.cudnn.deterministic = False\ntorch.backends.cudnn.allow_tf32 = True\ndata = torch.randn([4, 1024, 14, 14], dtype=torch.float, device='cuda', requires_grad=True)\nnet = torch.nn.Conv2d(1024, 256, kernel_size=[1, 1], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)\nnet = net.cuda().float()\nout = net(data)\nout.backward(torch.randn_like(out))\ntorch.cuda.synchronize()\n\nConvolutionParams \n    memory_format = Contiguous\n    data_type = CUDNN_DATA_FLOAT\n    padding = [0, 0, 0]\n    stride = [1, 1, 0]\n    dilation = [1, 1, 0]\n    groups = 1\n    deterministic = false\n    allow_tf32 = true\ninput: TensorDescriptor 00000179106BD3D0\n    type = CUDNN_DATA_FLOAT\n    nbDims = 4\n    dimA = 4, 1024, 14, 14, \n    strideA = 200704, 196, 14, 1, \noutput: TensorDescriptor 00000179106BD1A0\n    type = CUDNN_DATA_FLOAT\n    nbDims = 4\n    dimA = 4, 256, 14, 14, \n    strideA = 50176, 196, 14, 1, \nweight: FilterDescriptor 00000178CAEA70F0\n    type = CUDNN_DATA_FLOAT\n    tensor_format = CUDNN_TENSOR_NCHW\n    nbDims = 4\n    dimA = 256, 1024, 1, 1, \nPointer addresses: \n    input: 0000000968E00000\n    output: 0000000968AC4000\n    weight: 0000000904A00000\nAdditional pointer addresses: \n    grad_output: 0000000968AC4000\n    grad_input: 0000000968E00000\nBackward data algorithm: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 6.83 s (started: 2023-08-30 12:05:20 +03:00)\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for x, y in tqdm(train_loader, total=len(train_loader), desc=\"Training\", leave=False):\n",
    "    optimizer.zero_grad()\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    y_hat = model(x)\n",
    "    loss = loss_fn(y_hat, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    total_loss += loss.item()\n",
    "print(f\"Loss: {total_loss / len(train_loader)}\")\n",
    "print(f\"Accuracy: {accuray_fn(y_hat.argmax(1), y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([96., 94.,  2., 29., 74., 87.,  8., 60., 87.,  5., 96., 67., 29., 51.,\n",
      "        66., 50.], requires_grad=True) tensor([ 4., 67., 74., 26., 71., 68., 45.,  4., 24.,  4.,  4., 54., 74., 69.,\n",
      "         4., 92.], requires_grad=True)\n",
      "42660.555509334416\n",
      "tensor(0.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      3\u001b[0m accuracy \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m tqdm(train_loader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(train_loader), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining\u001b[39m\u001b[39m\"\u001b[39m, leave\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m      5\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(train_loader))\n\u001b[0;32m      6\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\DELL\\OneDrive\\Desktop\\fellowship_AI\\data_processing.py:68\u001b[0m, in \u001b[0;36mFlowerDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     66\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mopen(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_split\u001b[39m.\u001b[39miloc[index][\u001b[39m\"\u001b[39m\u001b[39mimage_path\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[1;32m---> 68\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m     69\u001b[0m classification \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_split\u001b[39m.\u001b[39miloc[index][\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m     70\u001b[0m classification \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(classification, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:94\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     93\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 94\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     95\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:349\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    342\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    344\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\functional.py:430\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    428\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    429\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 430\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, max_size\u001b[39m=\u001b[39;49mmax_size)\n\u001b[0;32m    432\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39msize, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, max_size\u001b[39m=\u001b[39mmax_size, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:282\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m     )\n\u001b[1;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], interpolation)\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\PIL\\Image.py:2079\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2075\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreducing_gap must be 1.0 or greater\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2077\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(size)\n\u001b[1;32m-> 2079\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   2080\u001b[0m \u001b[39mif\u001b[39;00m box \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2081\u001b[0m     box \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize\n",
      "File \u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\torch\\lib\\site-packages\\PIL\\ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> 260\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    261\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    262\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2min 15s (started: 2023-08-30 11:54:16 +03:00)\n"
     ]
    }
   ],
   "source": [
    "# for Epoch in range(20):\n",
    "#     total_loss = 0\n",
    "#     accuracy = 0\n",
    "#     for x, y in tqdm(train_loader, total=len(train_loader), desc=\"Training\", leave=False):\n",
    "#         x, y = next(iter(train_loader))\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device).type(torch.FloatTensor).requires_grad_(True)\n",
    "#         optimizer.zero_grad()\n",
    "#         y_hat = model(x).argmax(dim=1).type(torch.FloatTensor).requires_grad_(True)\n",
    "#         loss = loss_fn( y_hat, y)\n",
    "#         accuray = (y_hat == y).sum()/len(y)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         total_loss += loss.item()\n",
    "#     print(y, y_hat)\n",
    "#     print(total_loss/len(train_loader))\n",
    "#     print(accuray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
